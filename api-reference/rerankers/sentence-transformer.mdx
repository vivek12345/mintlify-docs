---
title: "Sentence Transformer Reranker"
description: "Local cross-encoder models for reranking"
---

## Overview

Sentence Transformer Reranker uses local cross-encoder models for reranking. It runs entirely on your infrastructure for privacy and cost efficiency.

## Setup

### Install Dependencies

Sentence Transformers is included with Mini RAG:

```bash
uv add mini-rag
```

## Configuration

### Basic Usage

```python
from mini import AgenticRAG, RerankerConfig

rag = AgenticRAG(
    vector_store=vector_store,
    embedding_model=embedding_model,
    reranker_config=RerankerConfig(
        type="sentence-transformer",
        kwargs={
            "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"
        }
    )
)
```

### With GPU

```python
reranker_config = RerankerConfig(
    type="sentence-transformer",
    kwargs={
        "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "device": "cuda"  # Use GPU
    }
)
```

### With CPU

```python
reranker_config = RerankerConfig(
    type="sentence-transformer",
    kwargs={
        "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "device": "cpu"  # Use CPU (slower)
    }
)
```

## Available Models

<AccordionGroup>
  <Accordion title="cross-encoder/ms-marco-MiniLM-L-6-v2">
    **Size**: Small (~80MB)  
    **Quality**: Good  
    **Speed**: Fast  
    **Best for**: General purpose, balanced performance
  </Accordion>
  
  <Accordion title="cross-encoder/ms-marco-MiniLM-L-12-v2">
    **Size**: Medium (~130MB)  
    **Quality**: Better  
    **Speed**: Moderate  
    **Best for**: Higher quality needs
  </Accordion>
  
  <Accordion title="cross-encoder/ms-marco-TinyBERT-L-2-v2">
    **Size**: Tiny (~40MB)  
    **Quality**: Basic  
    **Speed**: Very Fast  
    **Best for**: Resource-constrained environments
  </Accordion>
  
  <Accordion title="BAAI/bge-reranker-base">
    **Size**: Medium (~280MB)  
    **Quality**: High  
    **Speed**: Moderate  
    **Best for**: Multilingual support
  </Accordion>
  
  <Accordion title="BAAI/bge-reranker-large">
    **Size**: Large (~560MB)  
    **Quality**: Highest  
    **Speed**: Slower  
    **Best for**: Maximum quality
  </Accordion>
</AccordionGroup>

## Direct Usage

Use the reranker directly:

```python
from mini.reranker import SentenceTransformerReranker

# Initialize
reranker = SentenceTransformerReranker(
    model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
    device="cuda"
)

# Rerank documents
query = "What is machine learning?"
documents = [
    "Machine learning is a subset of AI...",
    "Python is a programming language...",
    "Deep learning uses neural networks..."
]

results = reranker.rerank(query, documents, top_k=2)

for result in results:
    print(f"Score: {result.score:.3f}")
    print(f"Document: {result.document[:100]}...")
```

## Complete Example

```python
import os
from mini import (
    AgenticRAG,
    LLMConfig,
    RetrievalConfig,
    RerankerConfig,
    EmbeddingModel,
    VectorStore
)

# Initialize RAG with local reranking
rag = AgenticRAG(
    vector_store=VectorStore(
        uri=os.getenv("MILVUS_URI"),
        token=os.getenv("MILVUS_TOKEN"),
        collection_name="documents",
        dimension=1536
    ),
    embedding_model=EmbeddingModel(),
    llm_config=LLMConfig(model="gpt-4o-mini"),
    retrieval_config=RetrievalConfig(
        top_k=10,
        rerank_top_k=3,
        use_reranking=True
    ),
    reranker_config=RerankerConfig(
        type="sentence-transformer",
        kwargs={
            "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2",
            "device": "cuda"
        }
    )
)

# Index and query
rag.index_document("document.pdf")
response = rag.query("What is the main topic?")

print(response.answer)
```

## Performance

### Speed Comparison

With 10 documents to rerank:

| Model | GPU (ms) | CPU (ms) |
|-------|----------|----------|
| TinyBERT-L-2 | 20-30 | 100-150 |
| MiniLM-L-6 | 30-50 | 150-250 |
| MiniLM-L-12 | 50-80 | 300-500 |
| bge-reranker-base | 60-100 | 400-600 |
| bge-reranker-large | 100-150 | 800-1200 |

### Memory Usage

| Model | GPU Memory | RAM |
|-------|------------|-----|
| TinyBERT-L-2 | ~200MB | ~500MB |
| MiniLM-L-6 | ~500MB | ~1GB |
| MiniLM-L-12 | ~800MB | ~1.5GB |
| bge-reranker-base | ~1.5GB | ~2.5GB |
| bge-reranker-large | ~2.5GB | ~4GB |

## Best Practices

<AccordionGroup>
  <Accordion title="Choose the Right Model">
    - **Prototyping**: MiniLM-L-6 (good balance)
    - **Production**: MiniLM-L-12 or bge-reranker-base
    - **High volume**: TinyBERT-L-2 (fastest)
    - **Best quality**: bge-reranker-large (if you have resources)
  </Accordion>
  
  <Accordion title="Use GPU When Possible">
    GPU provides 5-10x speedup:
    
    ```python
    import torch
    
    # Check GPU availability
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    reranker_config = RerankerConfig(
        type="sentence-transformer",
        kwargs={"device": device}
    )
    ```
  </Accordion>
  
  <Accordion title="Cache the Model">
    The model is loaded once and reused:
    
    ```python
    # Model loads on first use
    rag = AgenticRAG(..., reranker_config=config)
    
    # Subsequent queries reuse the model
    response1 = rag.query("question 1")
    response2 = rag.query("question 2")
    ```
  </Accordion>
  
  <Accordion title="Batch Processing">
    Process multiple queries to amortize model loading:
    
    ```python
    questions = [...]
    for question in questions:
        response = rag.query(question)
    ```
  </Accordion>
</AccordionGroup>

## Advantages

✅ **Privacy**: Runs entirely on your infrastructure  
✅ **No API Costs**: Free after model download  
✅ **No Rate Limits**: Process as many queries as you want  
✅ **Low Latency**: Fast with GPU  
✅ **Offline**: Works without internet

## Limitations

❌ **Initial Download**: Models need to be downloaded first  
❌ **Resource Requirements**: Needs GPU for best performance  
❌ **Quality**: Slightly lower than Cohere  
❌ **Maintenance**: You manage the infrastructure

## Troubleshooting

<AccordionGroup>
  <Accordion title="Model Download Fails">
    Models are downloaded from Hugging Face:
    ```python
    # Check connection
    from sentence_transformers import CrossEncoder
    
    model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    ```
  </Accordion>
  
  <Accordion title="CUDA Out of Memory">
    Use a smaller model or CPU:
    ```python
    # Smaller model
    reranker_config = RerankerConfig(
        type="sentence-transformer",
        kwargs={
            "model_name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
            "device": "cuda"
        }
    )
    
    # Or use CPU
    reranker_config = RerankerConfig(
        type="sentence-transformer",
        kwargs={"device": "cpu"}
    )
    ```
  </Accordion>
  
  <Accordion title="Slow Performance">
    Ensure GPU is being used:
    ```python
    import torch
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")
    ```
  </Accordion>
</AccordionGroup>

## See Also

<CardGroup cols={3}>
  <Card title="Rerankers Overview" href="/api-reference/rerankers/overview">
    Compare rerankers
  </Card>
  <Card title="Reranking Feature" href="/features/reranking">
    Learn about reranking
  </Card>
  <Card title="Sentence Transformers" href="https://www.sbert.net/">
    Official documentation
  </Card>
</CardGroup>

